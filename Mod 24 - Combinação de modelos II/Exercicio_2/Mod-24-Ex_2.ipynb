{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d746aa",
   "metadata": {},
   "source": [
    "### 1.Cite 5 diferenças entre o AdaBooste o GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd4af2",
   "metadata": {},
   "source": [
    "1. **Tipo de Função de Perda**:\n",
    "   - **AdaBoost**: Minimiza a **exponential loss** (função de perda exponencial), que penaliza fortemente as previsões incorretas.\n",
    "   - **GBM**: Permite a escolha de diferentes funções de perda, como **log_loss** para classificação ou **mean_squared_error** para regressão, oferecendo maior flexibilidade.\n",
    "\n",
    "2. **Peso das Amostras**:\n",
    "   - **AdaBoost**: Ajusta os pesos das amostras em cada iteração, aumentando o peso das amostras que foram classificadas incorretamente e diminuindo o peso das que foram classificadas corretamente.\n",
    "   - **GBM**: Ajusta os erros residuais em cada iteração, focando nas amostras com maiores erros residuais da iteração anterior para melhorar a precisão do modelo.\n",
    "\n",
    "3. **Método de Atualização do Modelo**:\n",
    "   - **AdaBoost**: Combina os modelos fracos (geralmente árvores de decisão) com base em uma ponderação que depende da precisão do modelo em cada iteração.\n",
    "   - **GBM**: Constrói novos modelos fracos que tentam corrigir os erros residuais do modelo anterior, acumulando gradualmente melhorias no modelo final.\n",
    "\n",
    "4. **Robustez ao Overfitting**:\n",
    "   - **AdaBoost**: Pode ser mais sensível ao overfitting, especialmente quando se usa um grande número de iterações e as amostras são ruidosas ou difíceis de classificar.\n",
    "   - **GBM**: Geralmente mais robusto ao overfitting, especialmente com o uso de técnicas como `subsample` e `learning_rate` para regularização.\n",
    "\n",
    "5. **Complexidade e Flexibilidade**:\n",
    "   - **AdaBoost**: Mais simples e fácil de implementar, mas com menos opções de personalização e ajuste fino.\n",
    "   - **GBM**: Mais complexo, mas oferece maior flexibilidade com muitos hiperparâmetros ajustáveis, como a taxa de aprendizado (`learning_rate`), o número de árvores (`n_estimators`), e a fração de amostras (`subsample`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e879f68",
   "metadata": {},
   "source": [
    "### 2.Acesse o link Scikit-learn–GBM, leia a explicação (traduza se for preciso) e crie um jupyternotebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb541c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb9839",
   "metadata": {},
   "source": [
    "####  Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49faff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707a6099",
   "metadata": {},
   "source": [
    "#### Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656e6eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43848663277068134"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.predict(X_test[1:2])\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4e790",
   "metadata": {},
   "source": [
    "### 3.Cite 5 Hyperparametrosimportantes no GBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfaf81d",
   "metadata": {},
   "source": [
    "1. **loss**: Define a função de perda a ser otimizada. `log_loss` é usada para saídas probabilísticas e é adequada para classificação, enquanto `exponential` usa o algoritmo AdaBoost.\n",
    "\n",
    "2. **learning_rate**: Controla o quanto cada árvore contribui para o modelo final. Uma taxa de aprendizado menor requer mais árvores (`n_estimators`) para alcançar uma boa performance, mas pode melhorar a generalização.\n",
    "\n",
    "3. **n_estimators**: Especifica o número de árvores (estágios de boosting) no modelo. Um número maior pode aumentar a performance, mas também aumenta o tempo de treinamento.\n",
    "\n",
    "4. **subsample**: Define a proporção das amostras utilizadas para treinar cada árvore. Usar um valor menor que 1.0 cria um modelo de boosting estocástico, reduzindo a variância e aumentando o viés.\n",
    "\n",
    "5. **criterion**: Determina como a qualidade das divisões das árvores é medida. O valor padrão `friedman_mse` é geralmente preferido, pois pode melhorar a performance em certos casos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f9337",
   "metadata": {},
   "source": [
    "### 4.(Opcional) Utilize o GridSearch para encontrar os melhores hyperparametrospara o conjunto de dados do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6458bfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(GradientBoostingRegressor(random_state=0),\n",
    "                           param_grid,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores hiperparâmetros: {grid_search.best_params_}\")\n",
    "\n",
    "best_reg = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e1fc5",
   "metadata": {},
   "source": [
    "### 5.Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao StochasticGBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87dfd9a",
   "metadata": {},
   "source": [
    "A maior diferença entre o **Gradient Boosting Machine (GBM)** tradicional e o **Stochastic Gradient Boosting Machine (Stochastic GBM)**, conforme descrito no artigo de Jerome Friedman, reside no uso de amostragem aleatória dos dados para treinar cada árvore de decisão no conjunto de árvores.\n",
    "\n",
    "### GBM Tradicional:\n",
    "- **Amostragem Completa**: No GBM tradicional, cada árvore de decisão é treinada usando **todo o conjunto de dados** disponível. Isso significa que em cada iteração do algoritmo, todos os dados de treinamento são utilizados para ajustar a árvore que minimiza o erro residual.\n",
    "  \n",
    "### Stochastic GBM:\n",
    "- **Amostragem Aleatória Parcial**: No Stochastic GBM, apenas uma **subamostra aleatória** dos dados é usada para treinar cada árvore. Essa subamostra é geralmente uma fração dos dados originais, definida pelo hiperparâmetro `subsample`. Por exemplo, se `subsample=0.8`, apenas 80% dos dados são usados para treinar cada árvore em uma iteração específica. \n",
    "\n",
    "### Consequências da Diferença:\n",
    "\n",
    "1. **Redução da Variância**: O uso de uma subamostra em cada iteração do Stochastic GBM introduz uma forma de regularização, que tende a reduzir a variância do modelo. Isso pode ajudar a evitar overfitting, especialmente em conjuntos de dados grandes ou ruidosos.\n",
    "\n",
    "2. **Aumento da Robustez**: Como cada árvore é treinada em uma amostra diferente dos dados, o modelo final (que é a média ponderada de todas as árvores) tende a ser mais robusto a outliers e a variações nos dados de treinamento.\n",
    "\n",
    "3. **Menor Tempo de Treinamento**: Treinar cada árvore em uma subamostra menor dos dados pode reduzir o tempo de treinamento em comparação ao GBM tradicional, que utiliza todo o conjunto de dados para cada árvore.\n",
    "\n",
    "### Resumo:\n",
    "A maior diferença entre o GBM tradicional e o Stochastic GBM é que o Stochastic GBM introduz aleatoriedade no processo de treinamento, usando apenas uma subamostra dos dados para treinar cada árvore, enquanto o GBM tradicional utiliza o conjunto completo de dados. Essa diferença pode levar a modelos mais robustos e menos propensos ao overfitting no caso do Stochastic GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957fce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
