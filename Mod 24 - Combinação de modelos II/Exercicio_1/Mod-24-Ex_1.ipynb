{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7157fd01",
   "metadata": {},
   "source": [
    "### 1. Cite 5 diferenças entre o Random Forest e o AdaBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac515a6",
   "metadata": {},
   "source": [
    "1. **Estratégia de Construção**:\n",
    "   - **Random Forest**: Constrói múltiplas árvores de decisão de forma independente, combinando suas previsões através de uma média (para regressão) ou voto majoritário (para classificação). Cada árvore é construída com um subconjunto aleatório dos dados de treinamento e um subconjunto aleatório de características.\n",
    "   - **AdaBoost**: Constrói uma sequência de árvores de decisão (ou outros classificadores fracos) de forma sequencial. Cada nova árvore é ajustada para corrigir os erros das árvores anteriores, aumentando o peso dos exemplos mal classificados.\n",
    "\n",
    "2. **Tipo de Modelo Base**:\n",
    "   - **Random Forest**: Utiliza árvores de decisão completas como modelos base. Cada árvore é treinada independentemente das outras.\n",
    "   - **AdaBoost**: Utiliza modelos fracos, frequentemente árvores de decisão rasas (com pouca profundidade, como árvores de decisão com um único nó de decisão).\n",
    "\n",
    "3. **Tratamento de Erros**:\n",
    "   - **Random Forest**: Não se preocupa diretamente com os erros das árvores individuais, confiando na agregação para reduzir o erro geral.\n",
    "   - **AdaBoost**: Dá mais peso aos exemplos que foram mal classificados por classificadores anteriores, focando na correção desses erros.\n",
    "\n",
    "4. **Resistência ao Overfitting**:\n",
    "   - **Random Forest**: Geralmente mais robusto ao overfitting devido ao uso de árvores de decisão independentes e à técnica de bootstrap (amostragem com reposição) dos dados de treinamento.\n",
    "   - **AdaBoost**: Pode ser mais suscetível ao overfitting, especialmente se os classificadores fracos forem muito complexos ou se o número de iterações for muito alto.\n",
    "\n",
    "5. **Combinação de Previsões**:\n",
    "   - **Random Forest**: Combina previsões através de votação majoritária para classificação ou média para regressão, sem priorizar árvores individuais.\n",
    "   - **AdaBoost**: Combina previsões ponderando cada classificador pela sua precisão, dando maior influência aos classificadores que melhoraram a performance nas iterações anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9c39c",
   "metadata": {},
   "source": [
    "### 2. Crie um Jupyter Notebook contendo o exemplo do AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10abdb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e3112f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666665"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85189b8d",
   "metadata": {},
   "source": [
    "### 3. Cite 5 hyperparametros importantes no AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3aeec5",
   "metadata": {},
   "source": [
    "1. **`estimator`**:\n",
    "   - **Descrição**: Classificador base para criar os *weak/base learners (Stumps)*. Por padrão, o estimador base é o algoritmo de árvore de decisão `DecisionTreeClassifier(max_depth=1)`. Sucessor do hiperparâmetro descontinuado `base_estimator`.\n",
    "   - **Importância**: Define o tipo de classificador fraco utilizado em cada iteração do AdaBoost.\n",
    "\n",
    "2. **`n_estimators`**:\n",
    "   - **Descrição**: Número máximo de iterações para finalizar o processo de aprendizagem. Pode ser interrompido antes deste limite caso seja considerado perfeito. Quanto maior o número de estimadores, maior será o tempo de treinamento. Também se refere à quantidade de *Stumps*.\n",
    "   - **Importância**: Aumentar o número de estimadores pode melhorar a performance, mas também aumenta o tempo de treinamento.\n",
    "\n",
    "3. **`learning_rate`**:\n",
    "   - **Descrição**: Valor do peso aplicado a cada classificador nas iterações de reforço. A relevância dos *Stumps* pode aumentar conforme essa definição, podendo haver uma compensação entre este hiperparâmetro e o `n_estimators`. Também conhecido como a taxa de aprendizado do AdaBoost.\n",
    "   - **Importância**: Controla a contribuição de cada classificador fraco, influenciando a robustez e a precisão do modelo.\n",
    "\n",
    "4. **`algorithm` (para `AdaBoostClassifier`)**:\n",
    "   - **Descrição**: Define o algoritmo utilizado para atualizar os pesos dos exemplos durante o treinamento. Pode ser `SAMME` (Discrete AdaBoost) ou `SAMME.R` (Real AdaBoost), onde `SAMME.R` é recomendado para melhor desempenho.\n",
    "   - **Importância**: O algoritmo selecionado pode afetar a eficiência e a performance do modelo, especialmente em problemas multiclasse.\n",
    "\n",
    "5. **`loss` (para `AdaBoostRegressor`)**:\n",
    "   - **Descrição**: Em problemas de regressão, este hiperparâmetro define a \"função de perda\" a ser usada ao atualizar os pesos após cada iteração de reforço. Pode ser `linear`, `square` ou `exponential`, tendo `linear` como padrão.\n",
    "   - **Importância**: A função de perda selecionada afeta como os erros são corrigidos e, consequentemente, a performance do regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990798f",
   "metadata": {},
   "source": [
    "### 4.(Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ee938d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e3bb039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados: {'estimator__max_depth': 3, 'learning_rate': 0.01, 'n_estimators': 50}\n",
      "Acurácia média com os melhores hiperparâmetros: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "base_clf = DecisionTreeClassifier()\n",
    "clf = AdaBoostClassifier(estimator=base_clf)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'estimator__max_depth': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Melhores hiperparâmetros encontrados:\", best_params)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "scores = cross_val_score(best_clf, X, y, cv=5)\n",
    "print(\"Acurácia média com os melhores hiperparâmetros:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f04fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
